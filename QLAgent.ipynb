{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# QL Agent #"
      ],
      "metadata": {
        "id": "XFdWl0X9eisQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toy environment."
      ],
      "metadata": {
        "id": "NrMGqW4reoY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import gym\n",
        "import itertools\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "from gym import error, spaces, utils\n",
        "from gym.utils import seeding\n",
        "from enum import Enum\n",
        "import plotting\n",
        "import collections\n",
        "from itertools import combinations\n",
        "  \n",
        "matplotlib.style.use('default')"
      ],
      "metadata": {
        "id": "XEqVAQf5DN4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Plant:\n",
        "    def __init__(self, species, maturity=10):\n",
        "        self.species = species\n",
        "        self.maturity = maturity         # consider 'days_to_maturity'\n",
        "        self.age = 0\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return \"{}\".format(self.species)\n",
        "    \n",
        "def truncate(n):\n",
        "    return int(n * 100) / 100\n",
        "class Field(gym.Env):\n",
        "    #metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, size=5, sow_limit=200, season=120, calendar=0):\n",
        "        # added to define action and observation spaces\n",
        "        self.action_space = spaces.Box(np.array([0,0,0]),\n",
        "                                       np.array([4,1,1]), dtype=np.float32)\n",
        "        \n",
        "        # reduced observation space to 1) standardize output and 2) remain consistent with genetic algo\n",
        "        self.observation_space = spaces.Box(np.array([0,0,0]),np.array([360,360,360]),dtype=np.int64)\n",
        "\n",
        "        # parameters for overall field character\n",
        "        self.size = size\n",
        "        self.sow_limit = sow_limit\n",
        "        self.season = season\n",
        "        self.calendar = calendar\n",
        "        \n",
        "        # constants for computing end-of-season reward---distances represent meters\n",
        "        self.crowding_dist = .02\n",
        "        self.maize_maize_dist = .1\n",
        "        self.bean_support_dist = .1\n",
        "        self.crowding_penalty = .1\n",
        "        self.maize_maize_penalty = .9\n",
        "        self.bean_support_bonus = .6\n",
        "        \n",
        "        ### self.observation_space = spaces.???\n",
        "        self.observation = [0,0,0]\n",
        "        # field is initialized by calling reset()\n",
        "        self.field = None\n",
        "    def step(self, action):\n",
        "        #for choice in action:\n",
        "        choice = action\n",
        "        if int(choice[0]) == 0:\n",
        "            self.field = np.append(self.field, [[self.size * truncate(choice[1]), \n",
        "                                              self.size * truncate(choice[2]), \n",
        "                                              Plant('Maize')]], axis=0)\n",
        "            self.observation[0] += 1\n",
        "        elif int(choice[0]) == 1:\n",
        "            self.field = np.append(self.field, [[self.size * truncate(choice[1]), \n",
        "                                              self.size * truncate(choice[2]), \n",
        "                                              Plant('Bean')]], axis=0)\n",
        "            self.observation[1] += 1\n",
        "        elif int(choice[0]) == 2:\n",
        "            self.field = np.append(self.field, [[self.size * truncate(choice[1]), \n",
        "                                              self.size * truncate(choice[2]), \n",
        "                                              Plant('Squash')]], axis=0)\n",
        "            self.observation[2] += 1\n",
        "        self.calendar +=1\n",
        "        for plant in self.field:\n",
        "            plant[2].age += 1\n",
        "            \n",
        "        done = self.calendar == self.season\n",
        "            \n",
        "        if not done:\n",
        "            reward = 0\n",
        "        else:\n",
        "            reward = self.get_reward()\n",
        "        #print(self.observation)\n",
        "        return self.field, reward, done, {}\n",
        "    \n",
        "    def reset(self):\n",
        "        # field is initialized with one random corn plant in order to make sowing (by np.append) work\n",
        "        self.field = np.array([[self.size * np.random.random(), \n",
        "                                self.size * np.random.random(), \n",
        "                                Plant('Maize')]])\n",
        "        # timekeeping is reset\n",
        "        self.calendar = 0\n",
        "        reward=0\n",
        "        self.observation=[0,0,0]\n",
        "        # added to avoid returning none type\n",
        "        return self.field\n",
        "        \n",
        "    def render(self, mode='human'):\n",
        "        # initialize plant type arrays so that pyplot won't break if any is empty\n",
        "        maize = np.array([[None, None]])\n",
        "        bean = np.array([[None, None]])\n",
        "        squash = np.array([[None, None]])\n",
        "        maize_imm = np.array([[None, None]])\n",
        "        bean_imm = np.array([[None, None]])\n",
        "        squash_imm = np.array([[None, None]])\n",
        "        # replace initial arrays with coordinates for each plant type; imm are plants that haven't matured\n",
        "        maize = np.array([row for row in self.field \n",
        "                            if row[2].__repr__() == 'Maize' and row[2].age >= row[2].maturity])\n",
        "        if maize.size==0:\n",
        "          maize = np.array([[None, None]])\n",
        "        bean = np.array([row for row in self.field \n",
        "                            if row[2].__repr__() == 'Bean' and row[2].age >= row[2].maturity])\n",
        "        if bean.size==0:\n",
        "          bean = np.array([[None, None]])\n",
        "        squash = np.array([row for row in self.field \n",
        "                              if row[2].__repr__() == 'Squash' and row[2].age >= row[2].maturity])\n",
        "        if squash.size==0:\n",
        "          squash = np.array([[None, None]])\n",
        "        maize_imm = np.array([row for row in self.field \n",
        "                            if row[2].__repr__() == 'Maize' and row[2].age < row[2].maturity])\n",
        "        if maize_imm.size==0:\n",
        "          maize_imm = np.array([[None, None]])\n",
        "        bean_imm = np.array([row for row in self.field \n",
        "                            if row[2].__repr__() == 'Bean' and row[2].age < row[2].maturity])\n",
        "        if bean_imm.size==0:\n",
        "          bean_imm = np.array([[None, None]])\n",
        "        squash_imm = np.array([row for row in self.field \n",
        "                            if row[2].__repr__() == 'Squash' and row[2].age < row[2].maturity])\n",
        "        if squash_imm.size==0:\n",
        "          squash_imm = np.array([[None, None]])\n",
        "        # plot the field---currently breaks if any plant type is absent\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.scatter(maize[:,0], maize[:,1], c='green', s=200, marker = 'o', alpha=.5, edgecolor='#303030')\n",
        "        plt.scatter(bean[:,0], bean[:,1], c='brown', s=150, marker = 'o', alpha=.5, edgecolor='#303030')\n",
        "        plt.scatter(squash[:,0], squash[:,1], c='orange', s=400, marker = 'o', alpha=.5, edgecolor='#303030')\n",
        "        plt.scatter(maize_imm[:,0], maize_imm[:,1], c='green', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
        "        plt.scatter(bean_imm[:,0], bean_imm[:,1], c='brown', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
        "        plt.scatter(squash_imm[:,0], squash_imm[:,1], c='orange', s=200, marker = 'o', alpha=.1, edgecolor='#303030')\n",
        "\n",
        "        plt.show()\n",
        "        \n",
        "        print(\"Total yield in Calories is {}.\\n---\\n\".format(round(self.get_reward(), 1)))\n",
        "    \n",
        "    def close(self):\n",
        "        # unneeded right now? AFAICT this is only used to shut down realtime movie visualizations\n",
        "        pass\n",
        "    \n",
        "    def get_reward(self):\n",
        "        # array of plant coordinates for computing distances\n",
        "        xy_array = np.array([[row[0], row[1]] for row in self.field])\n",
        "\n",
        "        # distances[m,n] is distance from mth to nth plant in field\n",
        "        distances = np.linalg.norm(xy_array - xy_array[:,None], axis=-1)\n",
        "        \n",
        "        reward = 0\n",
        "        i = 0\n",
        "        while i < len(self.field):\n",
        "            if self.field[i,2].age < self.field[i,2].maturity:\n",
        "                reward += 0\n",
        "            elif self.field[i,2].__repr__() == 'Maize':\n",
        "                cal = 1\n",
        "                j = 0\n",
        "                while j < len(distances[0]):\n",
        "                    if (self.field[j,2].__repr__() == 'Bean' \n",
        "                            and distances[i,j] < self.bean_support_dist):\n",
        "                        cal += self.bean_support_bonus\n",
        "                    if (self.field[j,2].__repr__() == 'Maize' \n",
        "                            and i !=j \n",
        "                            and distances[i,j] < self.maize_maize_dist):\n",
        "                        cal *= self.maize_maize_penalty\n",
        "                    if 0 < distances[i,j] < self.crowding_dist:\n",
        "                        cal *= self.crowding_penalty\n",
        "                    j += 1\n",
        "                reward += cal\n",
        "            elif self.field[i,2].__repr__() == 'Bean':\n",
        "                reward += .25\n",
        "            elif self.field[i,2].__repr__() == 'Squash':\n",
        "                reward += 3\n",
        "            i += 1        \n",
        "        return reward\n"
      ],
      "metadata": {
        "id": "qmVocJtimbe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Field()"
      ],
      "metadata": {
        "id": "j0k8uZqknviM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "metadata": {
        "id": "5XkqDtwi8EOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlO7WIQHu_7D"
      },
      "outputs": [],
      "source": [
        "# testing empty environment\n",
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing environment output using randomized action space"
      ],
      "metadata": {
        "id": "oIAOLRToe0MJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum = 0\n",
        "for i in tqdm(range(100)):  \n",
        "  episodes = 50 \n",
        "  rewardRandomList=[]\n",
        "  for episode in range(1, episodes+1):\n",
        "      state = env.reset()\n",
        "      done = False\n",
        "      score = 0 \n",
        "      while not done:\n",
        "          action = env.action_space.sample()\n",
        "          n_state, reward, done, info = env.step(action)\n",
        "          score+=reward\n",
        "      rewardRandomList.append(env.get_reward())\n",
        "  sum += max(rewardRandomList)\n",
        "print(sum/100)"
      ],
      "metadata": {
        "id": "QYht9mm_66Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "state = env.reset()\n",
        "done = False\n",
        "score = 0 \n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    n_state, reward, done, info = env.step(action)\n",
        "    score+=reward\n",
        "env.render()"
      ],
      "metadata": {
        "id": "YGIzuwNWg0x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states = env.observation_space.shape\n",
        "actions = env.action_space.shape"
      ],
      "metadata": {
        "id": "Nzv4cVOw_Vsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(states)\n",
        "print(actions)"
      ],
      "metadata": {
        "id": "xP5rWMjqAbK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin Q-Learning Agent"
      ],
      "metadata": {
        "id": "XTITGWAje4Ap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameters\n",
        "episodes = 50\n",
        "discount_factor = 1.3\n",
        "alpha = 0.8"
      ],
      "metadata": {
        "id": "JPvlKg08IvIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intializing Q Table\n",
        "actionToIndex= {}\n",
        "indexToAction={}\n",
        "cnt = 0\n",
        "for i in range(3):\n",
        "  for j in range(100):\n",
        "    for k in range(100):\n",
        "      action = [i,j/100, k/100]\n",
        "      actionToIndex[str(action)]=cnt\n",
        "      indexToAction[cnt]=action\n",
        "      cnt += 1\n"
      ],
      "metadata": {
        "id": "NCQQ9eC5YhqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filling Q Table\n",
        "Q=[]\n",
        "for i in range(120):\n",
        "   temp = []\n",
        "   for j in range(len(actionToIndex)):\n",
        "     temp.append(np.random.rand())\n",
        "   Q.append(temp)"
      ],
      "metadata": {
        "id": "Ymj9bu9nanMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining activation function\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ],
      "metadata": {
        "id": "EEhmqh8viO-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running QL\n",
        "actionSet = {}\n",
        "rewardList = []\n",
        "for i in tqdm(range(episodes)):\n",
        "  env.reset()\n",
        "  temp =0\n",
        "  #print(\"iter: \" + str(i))\n",
        "  for j in range(120):\n",
        "    #choose an action\n",
        "    actionIndex = np.argmax(softmax(Q[j])) \n",
        "    temp+=actionIndex\n",
        "    observation, reward, done, _ = env.step(indexToAction[actionIndex])\n",
        "    if j < 118:\n",
        "      best_next_action = np.argmax(Q[j+1])\n",
        "      td_target = reward + discount_factor * Q[j+1][best_next_action]\n",
        "      td_delta = td_target - Q[j][actionIndex]\n",
        "      before = Q[j][actionIndex]\n",
        "      Q[j][actionIndex]+= alpha * td_delta\n",
        "      print(\"j: \" + str(j) + \" actionIndex: \" + str(actionIndex)+ \" action: \" + str(indexToAction[actionIndex]) + \" before: \" + str(before))\n",
        "      print(Q[j][actionIndex])\n",
        "  rewardList.append(env.get_reward())\n",
        "  actionSet[i]=temp\n",
        "  \n",
        "  #env.render()\n"
      ],
      "metadata": {
        "id": "I-qM4UdNIhN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing reward over time\n",
        "matplotlib.pyplot.plot(range(episode), rewardList )"
      ],
      "metadata": {
        "id": "8rcpEAJOYYUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Rendering ptimal actions\n",
        "env.reset()\n",
        "for i in range(len(Q)):\n",
        "  action = np.argmax(Q[i])\n",
        "  env.step(indexToAction[action])\n",
        "env.render()"
      ],
      "metadata": {
        "id": "Hfx8JuYXfp20"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "QLAgent.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}